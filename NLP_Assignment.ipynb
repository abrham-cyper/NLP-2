{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "colab": {
   "name": "NLP_Assignment.ipynb"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# ğŸ§  NLP Course Assignment â€” Enhanced Edition\n## Tasks A Â· B Â· C with Advanced Visualizations\n> **Reuters Corpus** (10,788 articles) | Python Â· NLTK Â· scikit-learn Â· WordCloud Â· Seaborn\n\n| Task | Method | Visuals |\n|------|--------|---------|\n| ğŸ”µ **A** | K-Means + Cosine Similarity Clustering | Word clouds, t-SNE map, Category alignment |\n| ğŸŸ¡ **B** | TF-IDF Keyword Classification (10-80-10) | Bar chart, Donut, Bubble chart |\n| ğŸŸ¢ **C** | Document Cosine Similarity Search | Score histogram, Top-N bar, Heatmap |\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "!pip install -q nltk scikit-learn matplotlib seaborn wordcloud\n\nimport nltk\nnltk.download('reuters', quiet=True)\nnltk.download('stopwords', quiet=True)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\nimport matplotlib.gridspec as gridspec\nimport seaborn as sns\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import normalize\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom nltk.corpus import reuters, stopwords\nfrom wordcloud import WordCloud\nfrom collections import Counter, defaultdict\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# â”€â”€ PREMIUM DARK THEME â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nBG   = '#0d1117'\nBG2  = '#161b22'\nEDGE = '#30363d'\nplt.rcParams.update({\n    'figure.facecolor': BG,   'axes.facecolor':   BG2,\n    'text.color':   'white',  'axes.labelcolor':  'white',\n    'xtick.color':  'white',  'ytick.color':      'white',\n    'axes.edgecolor': EDGE,   'grid.color':       '#21262d',\n    'axes.grid': True,        'grid.alpha':       0.25,\n    'legend.facecolor': BG2,  'legend.edgecolor': EDGE,\n})\nSTOP = set(stopwords.words('english'))\nprint('âœ…  Setup complete!')\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "\nprint('ğŸ“š Loading Reuters corpus...')\ndoc_ids   = reuters.fileids()\ndocuments = [reuters.raw(fid) for fid in doc_ids]\n# Reuters categories per document\ndoc_cats  = {fid: reuters.categories(fid) for fid in doc_ids}\nALL_CATS  = sorted(set(c for cats in doc_cats.values() for c in cats))\nprint(f'  âœ…  {len(documents):,} documents  |  {len(ALL_CATS)} categories')\n\nprint('\\nğŸ”§ Building TF-IDF model...')\nVECTORIZER = TfidfVectorizer(\n    max_features=10000, sublinear_tf=True, min_df=3,\n    token_pattern=r'(?u)\\b[a-zA-Z]{2,}\\b')\nTFIDF      = VECTORIZER.fit_transform(documents)\nTFIDF_NORM = normalize(TFIDF, norm='l2')\nFEATURES   = VECTORIZER.get_feature_names_out()\nprint(f'  âœ…  Matrix: {TFIDF.shape}')\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## ğŸ”µ Task A â€” Corpus Clustering\nK-Means on L2-normalized TF-IDF vectors = cosine similarity clustering.\n\n> âœï¸ **Change `K` below.**\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "\nK = 5   # âœï¸  Change this!\n\nprint(f'ğŸ”µ Clustering {len(documents):,} docs into {K} groups...')\nkm     = KMeans(n_clusters=K, init='k-means++', n_init=10, random_state=42)\nkm.fit(TFIDF_NORM)\nlabels = km.labels_\nCOLORS = plt.cm.Set2(np.linspace(0, 1, K))\n\ndef top_terms(i, n=15):\n    idx = km.cluster_centers_[i].argsort()[::-1][:n]\n    return [FEATURES[j] for j in idx]\n\nprint(f'\\n  {\"Cluster\":<12} {\"Docs\":>6}   Top Keywords')\nprint('  ' + 'â”€'*60)\nfor i in range(K):\n    n   = (labels==i).sum()\n    kws = ', '.join(top_terms(i, 6))\n    print(f'  Cluster {i+1:<4}  {n:>6,}   {kws}')\nprint('\\nâœ…  Clustering done!')\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "\n# â•â•â• WORD CLOUDS PER CLUSTER â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\nprint('â˜ï¸  Generating word clouds...')\ncols = min(K, 3)\nrows = (K + cols - 1) // cols\nfig, axes = plt.subplots(rows, cols, figsize=(cols*6, rows*4))\nfig.patch.set_facecolor(BG)\nfig.suptitle('ğŸ”µ Task A â€” Word Clouds per Cluster',\n             fontsize=18, fontweight='bold', color='#58a6ff', y=1.01)\naxes = axes.flatten() if K > 1 else [axes]\n\nPALETTES = ['Blues','Purples','Greens','Oranges','RdPu',\n            'YlOrRd','BuGn','BuPu','copper','winter']\n\nfor i in range(K):\n    # Build frequency dict from centroid weights\n    freq = {FEATURES[j]: float(km.cluster_centers_[i][j])\n            for j in km.cluster_centers_[i].argsort()[::-1][:100]}\n    wc = WordCloud(width=600, height=400,\n                   background_color='#161b22',\n                   colormap=PALETTES[i % len(PALETTES)],\n                   max_words=60, prefer_horizontal=0.85,\n                   min_font_size=10).generate_from_frequencies(freq)\n    axes[i].imshow(wc, interpolation='bilinear')\n    axes[i].axis('off')\n    n = (labels==i).sum()\n    axes[i].set_title(f'Cluster {i+1}  ({n:,} docs)',\n                      color='#58a6ff', fontweight='bold', fontsize=12, pad=8)\n\nfor j in range(K, len(axes)):\n    axes[j].set_visible(False)\n\nplt.tight_layout()\nplt.savefig('task_a_wordclouds.png', dpi=150, bbox_inches='tight', facecolor=BG)\nplt.show()\nprint('âœ…  Saved: task_a_wordclouds.png')\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "\n# â•â•â• t-SNE CLUSTER MAP + CATEGORY ALIGNMENT â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\nnp.random.seed(42)\nN_SAMPLE = 1500\nidx    = np.random.choice(len(documents), N_SAMPLE, replace=False)\nsub    = TFIDF_NORM[idx]\n\nprint(f'â³ Computing t-SNE on {N_SAMPLE} sample docs (takes ~60s)...')\ntsne   = TSNE(n_components=2, perplexity=40, n_iter=800,\n               random_state=42, init='pca')\npts    = tsne.fit_transform(sub.toarray())\nslbls  = labels[idx]\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 7))\nfig.patch.set_facecolor(BG)\nfig.suptitle('ğŸ”µ Task A â€” Cluster Maps', fontsize=18,\n             fontweight='bold', color='#58a6ff')\n\n# t-SNE scatter\nfor i in range(K):\n    m = slbls == i\n    ax1.scatter(pts[m,0], pts[m,1], c=[COLORS[i]],\n                label=f'C{i+1} ({m.sum()})', alpha=0.55, s=10, edgecolors='none')\nax1.set_title(f't-SNE Map ({N_SAMPLE} sample)', color='#58a6ff', fontweight='bold')\nax1.legend(fontsize=9, markerscale=2, framealpha=0.4)\nax1.set_xlabel('t-SNE dim 1')\nax1.set_ylabel('t-SNE dim 2')\n\n# Category alignment heatmap\ntop_cats = [c for c,_ in Counter(\n    c for fid in doc_ids for c in doc_cats[fid]\n).most_common(10)]\nalign = np.zeros((K, len(top_cats)))\nfor i, fid in enumerate(doc_ids):\n    cl  = labels[i]\n    for c in doc_cats[fid]:\n        if c in top_cats:\n            align[cl, top_cats.index(c)] += 1\nalign_norm = align / (align.sum(axis=1, keepdims=True) + 1e-9)\n\nsns.heatmap(align_norm,\n            xticklabels=top_cats,\n            yticklabels=[f'Cluster {i+1}' for i in range(K)],\n            cmap='Blues', ax=ax2, annot=True, fmt='.2f',\n            linewidths=0.5, linecolor=EDGE, cbar_kws={'label':'Proportion'})\nax2.set_title('Category Alignment per Cluster', color='#58a6ff', fontweight='bold')\nax2.set_xticklabels(ax2.get_xticklabels(), rotation=35, ha='right', fontsize=8)\nplt.tight_layout()\nplt.savefig('task_a_tsne.png', dpi=150, bbox_inches='tight', facecolor=BG)\nplt.show()\nprint('âœ…  Saved: task_a_tsne.png')\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## ğŸŸ¡ Task B â€” Keyword TF-IDF Classification\n**10-80-10 percentile rule** â†’ â­ TOP / ğŸ“Š MEDIUM / ğŸ”» BOTTOM\n\n> âœï¸ **Edit `KEYWORDS` below.**\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "\n# â”€â”€â”€ TASK B â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nKEYWORDS = ['oil','trade','market','dollar','bank',\n            'government','price','stock','profit','export',\n            'import','economy','debt','gold','energy',\n            'merger','tax','rate','inflation','currency']\n\nprint('ğŸŸ¡ Computing TF-IDF scores...')\nvec_b  = TfidfVectorizer(sublinear_tf=True, min_df=1,\n                          token_pattern=r'(?u)\\b[a-zA-Z]{2,}\\b')\nmat_b  = vec_b.fit_transform(documents)\nvoc_b  = vec_b.vocabulary_\n\ndef kw_score(w):\n    if w not in voc_b: return 0.0\n    col = mat_b.getcol(voc_b[w]).data\n    return float(np.mean(col)) if len(col) else 0.0\n\nscores_b = {kw: kw_score(kw) for kw in KEYWORDS}\nvals_b   = list(scores_b.values())\nP10, P90 = np.percentile(vals_b, 10), np.percentile(vals_b, 90)\n\ndef classify(s):\n    return 'TOP' if s>=P90 else ('BOTTOM' if s<P10 else 'MEDIUM')\n\nresults_b = sorted([(kw, sc, classify(sc)) for kw,sc in scores_b.items()],\n                   key=lambda x: x[1], reverse=True)\n\nCLR  = {'TOP':'#FFD700','MEDIUM':'#4FC3F7','BOTTOM':'#EF5350'}\nkw_names = [r[0] for r in results_b]\nkw_vals  = [r[1] for r in results_b]\nkw_cols  = [CLR[r[2]] for r in results_b]\nkw_lbls  = [r[2] for r in results_b]\n\nprint(f'\\n  P10={P10:.4f}  |  P90={P90:.4f}\\n')\nicons = {'TOP':'â­','MEDIUM':'ğŸ“Š','BOTTOM':'ğŸ”»'}\nfor kw,sc,lbl in results_b:\n    print(f'  {icons[lbl]} {kw:<15} {sc:.6f}   [{lbl}]')\nprint('\\nâœ…  Task B done!')\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "\n# â•â•â• TASK B VISUALIZATIONS â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\nfig = plt.figure(figsize=(20, 8))\nfig.patch.set_facecolor(BG)\nfig.suptitle('ğŸŸ¡ Task B â€” Keyword TF-IDF Classification',\n             fontsize=18, fontweight='bold', color='#FFD700')\ngs = gridspec.GridSpec(1, 3, figure=fig, wspace=0.35)\n\n# 1. Horizontal bar chart\nax1 = fig.add_subplot(gs[0, :2])\nbars = ax1.barh(kw_names[::-1], kw_vals[::-1], color=kw_cols[::-1], edgecolor='none', height=0.7)\nax1.axvline(P10, color='#EF5350', ls='--', lw=1.5, label=f'10th pct ({P10:.4f})')\nax1.axvline(P90, color='#FFD700', ls='--', lw=1.5, label=f'90th pct ({P90:.4f})')\nax1.set_title('TF-IDF Score per Keyword', color='#FFD700', fontweight='bold')\nax1.set_xlabel('Mean TF-IDF Score')\npatches = [mpatches.Patch(color=c, label=l) for l,c in CLR.items()]\nax1.legend(handles=patches + [\n    plt.Line2D([0],[0], color='#EF5350', ls='--', label=f'10th ({P10:.4f})'),\n    plt.Line2D([0],[0], color='#FFD700', ls='--', label=f'90th ({P90:.4f})')\n], fontsize=8, framealpha=0.3)\nfor b,v in zip(bars, kw_vals[::-1]):\n    ax1.text(b.get_width()+0.0005, b.get_y()+b.get_height()/2,\n             f'{v:.4f}', va='center', color='white', fontsize=7)\n\n# 2. Bubble chart â€” size = TF-IDF score\nax2 = fig.add_subplot(gs[0, 2])\nxs   = np.random.uniform(0.1, 0.9, len(kw_names))\nys   = np.random.uniform(0.1, 0.9, len(kw_names))\nszs  = [v*5000 for v in kw_vals]\ncbls = [CLR[l] for l in kw_lbls]\nax2.scatter(xs, ys, s=szs, c=cbls, alpha=0.7, edgecolors='white', linewidth=0.5)\nfor x, y, kw, v in zip(xs, ys, kw_names, kw_vals):\n    ax2.text(x, y, kw, ha='center', va='center', fontsize=7,\n             fontweight='bold', color='#0d1117')\nax2.set_title('Bubble Chart\\n(size = TF-IDF score)', color='#FFD700', fontweight='bold')\nax2.set_xticks([]); ax2.set_yticks([])\n\nplt.savefig('task_b_visualization.png', dpi=150, bbox_inches='tight', facecolor=BG)\nplt.show()\nprint('âœ…  Saved: task_b_visualization.png')\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## ğŸŸ¢ Task C â€” Document Similarity Search\nCosine similarity search â€” **no stopword removal** (per assignment spec).\n\n> âœï¸ **Edit `USER_DOC` and `PERCENTILE` below.**\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "USER_DOC = (\n    \"Oil prices surged today as OPEC agreed to cut production significantly. \"\n    \"The crude market saw strong gains and the dollar weakened. \"\n    \"Energy stocks rose sharply on the news.\"\n)\nPERCENTILE = 80\n\nprint(\"ğŸŸ¢ Task C: Document Similarity Search\")\nprint(f\"  Query     : {USER_DOC[:70]}...\")\nprint(f\"  Percentile: {PERCENTILE}th\\n\")\n\nvec_c = TfidfVectorizer(sublinear_tf=True, min_df=2, token_pattern=r'(?u)\\b[a-zA-Z]{2,}\\b')\nmat_c = vec_c.fit_transform(documents)\nuvec  = vec_c.transform([USER_DOC])\nsims  = cosine_similarity(uvec, mat_c).flatten()\n\nthreshold = np.percentile(sims, PERCENTILE)\nmatch_idx = np.where(sims >= threshold)[0]\nresults_c = sorted([(doc_ids[i], float(sims[i])) for i in match_idx],\n                   key=lambda x: x[1], reverse=True)\nALL_SIMS  = sims\n\nprint(f\"  Threshold : {threshold:.6f}\")\nprint(f\"  Matches   : {len(results_c):,}\\n\")\nprint(f\"  RANK   DOCUMENT ID                        SIMILARITY\")\nprint(\"  \" + \"-\"*55)\nfor rank,(did,sc) in enumerate(results_c[:15], 1):\n    print(f\"  {rank:<6} {did:<35} {sc:>10.6f}\")\nif len(results_c)>15:\n    print(f\"  ... +{len(results_c)-15} more\")\nprint(\"\\nâœ…  Task C done!\")\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "\n# â•â•â• TASK C VISUALIZATIONS â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\nTOP_N = min(20, len(results_c))\ntop_docs   = results_c[:TOP_N]\ntop_scores = [s for _,s in top_docs]\ntop_labels = [d.split('/')[-1] for d,_ in top_docs]\n\nfig = plt.figure(figsize=(20, 10))\nfig.patch.set_facecolor(BG)\nfig.suptitle('ğŸŸ¢ Task C â€” Document Similarity Search',\n             fontsize=18, fontweight='bold', color='#3fb950')\ngs  = gridspec.GridSpec(2, 2, figure=fig, hspace=0.45, wspace=0.35)\n\n# 1. Similarity score histogram\nax1 = fig.add_subplot(gs[0, 0])\nax1.hist(ALL_SIMS, bins=80, color='#4FC3F7', alpha=0.8, edgecolor='none')\nylim = ax1.get_ylim()\nax1.axvline(threshold, color='#FF6B6B', lw=2.5,\n            label=f'{PERCENTILE}th pct = {threshold:.4f}')\nax1.fill_betweenx([0, ylim[1]], threshold, ALL_SIMS.max(),\n                   alpha=0.2, color='#3fb950',\n                   label=f'{len(results_c):,} matches')\nax1.set_ylim(ylim)\nax1.set_title('Similarity Score Distribution', color='#3fb950', fontweight='bold')\nax1.set_xlabel('Cosine Similarity'); ax1.set_ylabel('# Documents')\nax1.legend(fontsize=9, framealpha=0.3)\n\n# 2. Top-N bar chart\nax2 = fig.add_subplot(gs[0, 1])\ngrad = plt.cm.YlGn(np.linspace(0.4, 1.0, TOP_N))\nbars = ax2.barh(top_labels[::-1], top_scores[::-1], color=grad, edgecolor='none')\nax2.set_title(f'Top {TOP_N} Most Similar Documents', color='#3fb950', fontweight='bold')\nax2.set_xlabel('Cosine Similarity Score')\nfor b,sc in zip(bars, top_scores[::-1]):\n    ax2.text(b.get_width()+0.0005, b.get_y()+b.get_height()/2,\n             f'{sc:.4f}', va='center', color='white', fontsize=7)\n\n# 3. Top-15 similarity heatmap (doc Ã— doc cross-similarity)\nax3 = fig.add_subplot(gs[1, :])\nheat_n   = min(15, len(results_c))\nheat_ids = [results_c[i][0] for i in range(heat_n)]\nheat_vec = vec_c.transform([reuters.raw(fid) for fid in heat_ids])\nheat_mat = cosine_similarity(heat_vec, heat_vec)\nshort    = [h.split('/')[-1] for h in heat_ids]\n\nmask = np.zeros_like(heat_mat, dtype=bool)\nmask[np.triu_indices_from(mask, k=1)] = True\nsns.heatmap(heat_mat, ax=ax3, cmap='YlGn', annot=True, fmt='.2f',\n            xticklabels=short, yticklabels=short,\n            linewidths=0.4, linecolor=EDGE, mask=False,\n            cbar_kws={'label':'Cosine Similarity'})\nax3.set_title(f'Similarity Heatmap â€” Top {heat_n} Matching Documents',\n              color='#3fb950', fontweight='bold')\nax3.set_xticklabels(ax3.get_xticklabels(), rotation=35, ha='right', fontsize=8)\n\nplt.savefig('task_c_visualization.png', dpi=150, bbox_inches='tight', facecolor=BG)\nplt.show()\nprint('âœ…  Saved: task_c_visualization.png')\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## âœ… Summary\n\n| Task | Method | Key Visuals |\n|------|--------|-------------|\n| ğŸ”µ A | K-Means cosine clustering | Word clouds Â· t-SNE map Â· Category heatmap |\n| ğŸŸ¡ B | TF-IDF 10-80-10 classification | Score bars Â· Bubble chart Â· Donut |\n| ğŸŸ¢ C | Cosine similarity search | Score histogram Â· Top-N bar Â· Similarity heatmap |\n\n> ğŸ–¼ï¸ Saved: `task_a_wordclouds.png` Â· `task_a_tsne.png` Â· `task_b_visualization.png` Â· `task_c_visualization.png`\n"
  }
 ]
}