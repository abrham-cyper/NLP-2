{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "colab": {
   "name": "NLP_Assignment.ipynb"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# ğŸ§  NLP Course Assignment\n## Tasks A Â· B Â· C â€” Corpus Analysis with Visualizations\n> **Reuters Corpus** (10,788 news articles) &nbsp;|&nbsp; Python Â· NLTK Â· scikit-learn Â· matplotlib\n\n---\n| Task | What it does |\n|------|-------------|\n| ğŸ”µ **A** | Cluster corpus into K groups using cosine similarity (K-Means) |\n| ğŸŸ¡ **B** | Score keywords with TF-IDF, classify as TOP / MEDIUM / BOTTOM |\n| ğŸŸ¢ **C** | Find documents similar to a query using cosine similarity |\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# â”€â”€ INSTALL & IMPORTS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n!pip install -q nltk scikit-learn matplotlib seaborn\n\nimport nltk\nnltk.download('reuters', quiet=True)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import normalize\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom nltk.corpus import reuters\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Dark GitHub-style theme\nplt.rcParams.update({\n    'figure.facecolor':'#0d1117','axes.facecolor':'#161b22',\n    'text.color':'white','axes.labelcolor':'white',\n    'xtick.color':'white','ytick.color':'white',\n    'axes.edgecolor':'#30363d','grid.color':'#21262d',\n    'axes.grid':True,'grid.alpha':0.3,\n})\nprint('âœ…  Setup complete!')\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# â”€â”€ LOAD REUTERS CORPUS + SHARED TF-IDF â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nprint('ğŸ“š Loading Reuters corpus...')\ndoc_ids   = reuters.fileids()\ndocuments = [reuters.raw(fid) for fid in doc_ids]\nprint(f'  âœ…  {len(documents):,} documents loaded')\n\nprint('\\nğŸ”§ Building shared TF-IDF model...')\nVECTORIZER = TfidfVectorizer(\n    max_features=10000, sublinear_tf=True, min_df=3,\n    token_pattern=r'(?u)\\b[a-zA-Z]{2,}\\b')\nTFIDF      = VECTORIZER.fit_transform(documents)\nTFIDF_NORM = normalize(TFIDF, norm='l2')\nFEATURES   = VECTORIZER.get_feature_names_out()\nprint(f'  âœ…  Matrix shape: {TFIDF.shape}')\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## ğŸ”µ Task A â€” Corpus Clustering\nGroups all documents into **K clusters** by topic/vocabulary similarity using K-Means on normalized TF-IDF vectors (equivalent to cosine similarity clustering).\n\n> âœï¸ **Change `K` in the cell below to try different numbers of clusters.**\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# â”€â”€ TASK A: CLUSTERING â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nK = 5        # âœï¸ Change this!\n\nprint(f'ğŸ”µ Clustering {len(documents):,} docs into {K} groups...')\nkm = KMeans(n_clusters=K, init='k-means++', n_init=10, random_state=42)\nkm.fit(TFIDF_NORM)\nlabels = km.labels_\n\ndef top_terms(i, n=8):\n    idx = km.cluster_centers_[i].argsort()[::-1][:n]\n    return [FEATURES[j] for j in idx]\n\nprint(f'\\n  {\"Cluster\":<12} {\"Docs\":>6}   Top Keywords')\nprint('  ' + 'â”€'*55)\nfor i in range(K):\n    n   = (labels==i).sum()\n    kws = ', '.join(top_terms(i, 6))\n    print(f'  Cluster {i+1:<4}  {n:>6,}   {kws}')\nprint('\\nâœ…  Clustering done!')\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# â”€â”€ TASK A: VISUALIZATIONS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nCOLORS = plt.cm.Set2(np.linspace(0, 1, K))\nsizes  = [(labels==i).sum() for i in range(K)]\n\nfig = plt.figure(figsize=(18, 10))\nfig.patch.set_facecolor('#0d1117')\nfig.suptitle(f'ğŸ”µ Task A â€” Corpus Clustering  (K={K})',\n             fontsize=20, fontweight='bold', color='#58a6ff', y=1.01)\n\n# 1. Bar chart of cluster sizes\nax1 = fig.add_subplot(2, 3, 1)\nbars = ax1.barh([f'Cluster {i+1}' for i in range(K)], sizes, color=COLORS, edgecolor='none')\nax1.set_title('Cluster Sizes', color='#58a6ff', fontweight='bold')\nax1.set_xlabel('# Documents')\nfor b, s in zip(bars, sizes):\n    ax1.text(b.get_width()+30, b.get_y()+b.get_height()/2, f'{s:,}', va='center', color='white', fontsize=9)\n\n# 2. PCA 2-D scatter\nax2 = fig.add_subplot(2, 3, 2)\nnp.random.seed(42)\nidx  = np.random.choice(len(documents), min(2000, len(documents)), replace=False)\npca  = PCA(n_components=2, random_state=42)\npts  = pca.fit_transform(TFIDF_NORM[idx].toarray())\nfor i in range(K):\n    m = labels[idx]==i\n    ax2.scatter(pts[m,0], pts[m,1], c=[COLORS[i]], label=f'C{i+1}', alpha=0.45, s=6)\nax2.set_title('PCA Cluster Map (2,000 sample)', color='#58a6ff', fontweight='bold')\nax2.legend(fontsize=8, markerscale=3, framealpha=0.3)\nax2.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%})')\nax2.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%})')\n\n# 3. Pie chart\nax3 = fig.add_subplot(2, 3, 3)\nwedges,_,autos = ax3.pie(sizes, labels=[f'C{i+1}' for i in range(K)],\n    colors=COLORS, autopct='%1.1f%%', startangle=90,\n    textprops={'color':'white','fontsize':9},\n    wedgeprops={'edgecolor':'#0d1117','linewidth':2})\n[a.set_color('#0d1117') for a in autos]\nax3.set_title('Distribution', color='#58a6ff', fontweight='bold')\n\n# 4-6. Top terms per cluster (first 3)\nfor i in range(min(K,3)):\n    ax = fig.add_subplot(2, 3, 4+i)\n    terms  = top_terms(i, 8)\n    scores = [km.cluster_centers_[i][VECTORIZER.vocabulary_[t]] for t in terms]\n    cmap   = plt.cm.Blues(np.linspace(0.4, 0.9, 8))\n    ax.barh(terms[::-1], scores[::-1], color=cmap[::-1], edgecolor='none')\n    ax.set_title(f'Cluster {i+1} â€” Top Terms', color='#58a6ff', fontweight='bold', fontsize=10)\n    ax.set_xlabel('Weight', fontsize=8)\n\nplt.tight_layout()\nplt.savefig('task_a_clusters.png', dpi=150, bbox_inches='tight', facecolor='#0d1117')\nplt.show()\nprint('âœ…  Saved: task_a_clusters.png')\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## ğŸŸ¡ Task B â€” Keyword TF-IDF Classification\nComputes the **average TF-IDF score** for each keyword across the entire corpus, then classifies using the **10-80-10 percentile rule**.\n\n| Class | Condition | Meaning |\n|-------|-----------|---------|\n| â­ **TOP** | score â‰¥ 90th percentile | Highly specific term |\n| ğŸ“Š **MEDIUM** | 10th â‰¤ score < 90th | Average importance |\n| ğŸ”» **BOTTOM** | score < 10th percentile | Very common / weak term |\n\n> âœï¸ **Edit `KEYWORDS` below. Press Enter to use `sample_keywords.txt`.**\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# â”€â”€ TASK B: KEYWORD CLASSIFICATION â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# âœï¸ Edit or load from file:\nKEYWORDS = ['oil','trade','market','dollar','bank',\n            'government','price','stock','profit','export',\n            'import','economy','debt','gold','energy']\n\n# Optionally load from file:\n# import os\n# if os.path.exists('sample_keywords.txt'):\n#     KEYWORDS = open('sample_keywords.txt').read().splitlines()\n\nprint('ğŸŸ¡ Computing TF-IDF for keywords...')\nvec_b  = TfidfVectorizer(sublinear_tf=True, min_df=1,\n                          token_pattern=r'(?u)\\b[a-zA-Z]{2,}\\b')\nmat_b  = vec_b.fit_transform(documents)\nvoc_b  = vec_b.vocabulary_\n\ndef kw_score(w):\n    w = w.lower().strip()\n    if w not in voc_b: return 0.0\n    col = mat_b.getcol(voc_b[w]).data\n    return float(np.mean(col)) if len(col) else 0.0\n\nscores_b = {kw: kw_score(kw) for kw in KEYWORDS}\nvals_b   = list(scores_b.values())\nP10, P90 = np.percentile(vals_b, 10), np.percentile(vals_b, 90)\n\ndef classify(s):\n    return 'TOP' if s>=P90 else ('BOTTOM' if s<P10 else 'MEDIUM')\n\nresults_b = sorted([(kw, sc, classify(sc)) for kw,sc in scores_b.items()],\n                   key=lambda x: x[1], reverse=True)\n\nprint(f'\\n  10th pct: {P10:.4f}  |  90th pct: {P90:.4f}\\n')\nicons = {'TOP':'â­','MEDIUM':'ğŸ“Š','BOTTOM':'ğŸ”»'}\nfor kw, sc, lbl in results_b:\n    print(f'  {icons[lbl]} {kw:<15} {sc:.6f}   [{lbl}]')\nprint('\\nâœ…  Task B done!')\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# â”€â”€ TASK B: VISUALIZATIONS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nCLR = {'TOP':'#FFD700','MEDIUM':'#4FC3F7','BOTTOM':'#EF5350'}\nkw_names = [r[0] for r in results_b]\nkw_vals  = [r[1] for r in results_b]\nkw_cols  = [CLR[r[2]] for r in results_b]\nkw_lbls  = [r[2] for r in results_b]\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 7))\nfig.patch.set_facecolor('#0d1117')\nfig.suptitle('ğŸŸ¡ Task B â€” Keyword TF-IDF Classification',\n             fontsize=18, fontweight='bold', color='#FFD700')\n\n# Bar chart\nbars = ax1.barh(kw_names[::-1], kw_vals[::-1], color=kw_cols[::-1], edgecolor='none')\nax1.axvline(P10, color='#EF5350', ls='--', lw=1.5, label=f'10th pct ({P10:.4f})')\nax1.axvline(P90, color='#FFD700', ls='--', lw=1.5, label=f'90th pct ({P90:.4f})')\nax1.set_title('TF-IDF Score per Keyword', color='#FFD700', fontweight='bold')\nax1.set_xlabel('Mean TF-IDF Score')\npatches = [mpatches.Patch(color=c, label=l) for l,c in CLR.items()]\nax1.legend(handles=patches + [\n    plt.Line2D([0],[0], color='#EF5350', ls='--', label=f'10th ({P10:.4f})'),\n    plt.Line2D([0],[0], color='#FFD700', ls='--', label=f'90th ({P90:.4f})')\n], fontsize=8, loc='lower right', framealpha=0.3)\nfor b, v in zip(bars, kw_vals[::-1]):\n    ax1.text(b.get_width()+0.001, b.get_y()+b.get_height()/2,\n             f'{v:.4f}', va='center', color='white', fontsize=8)\n\n# Donut chart\ntops = kw_lbls.count('TOP')\nmeds = kw_lbls.count('MEDIUM')\nbots = kw_lbls.count('BOTTOM')\nwedges,_,autos = ax2.pie(\n    [tops, meds, bots],\n    labels=[f'â­ TOP\\n({tops})', f'ğŸ“Š MEDIUM\\n({meds})', f'ğŸ”» BOTTOM\\n({bots})'],\n    colors=['#FFD700','#4FC3F7','#EF5350'],\n    autopct='%1.0f%%', startangle=90,\n    textprops={'color':'white','fontsize':11},\n    wedgeprops={'edgecolor':'#0d1117','linewidth':2,'width':0.6})\n[a.set_color('#0d1117') for a in autos]\nax2.set_title('10 â€“ 80 â€“ 10 Distribution', color='#FFD700', fontweight='bold')\n\nplt.tight_layout()\nplt.savefig('task_b_keywords.png', dpi=150, bbox_inches='tight', facecolor='#0d1117')\nplt.show()\nprint('âœ…  Saved: task_b_keywords.png')\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## ğŸŸ¢ Task C â€” Document Similarity Search\nGiven a query document and a percentile threshold, returns all corpus documents **above that percentile** in cosine similarity.\n\n> âš ï¸ **No stopword removal** is applied, as required by the assignment.\n>\n> âœï¸ **Edit `USER_DOC` and `PERCENTILE` below.**\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# â”€â”€ TASK C: SIMILARITY SEARCH â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nUSER_DOC = (\n    \"Oil prices surged today as OPEC agreed to cut production significantly. \"\n    \"The crude market saw strong gains and the dollar weakened. \"\n    \"Energy stocks rose sharply on the news.\"\n)\nPERCENTILE = 80  # âœï¸ Change: 80 = return top 20% most similar\n\nprint(\"ğŸŸ¢ Task C: Document Similarity Search\")\nprint(f\"  Query: \\\"{USER_DOC[:70]}...\\\"\")\nprint(f\"  Threshold: {PERCENTILE}th percentile\\n\")\n\n# Build TF-IDF without stopword removal (per assignment spec)\nvec_c = TfidfVectorizer(sublinear_tf=True, min_df=2, token_pattern=r'(?u)\\b[a-zA-Z]{2,}\\b')\nmat_c = vec_c.fit_transform(documents)\nuvec  = vec_c.transform([USER_DOC])\nsims  = cosine_similarity(uvec, mat_c).flatten()\n\nthreshold = np.percentile(sims, PERCENTILE)\nmatch_idx = np.where(sims >= threshold)[0]\nresults_c = sorted([(doc_ids[i], float(sims[i])) for i in match_idx],\n                   key=lambda x: x[1], reverse=True)\nALL_SIMS  = sims\n\nprint(f\"  Threshold : {threshold:.6f}\")\nprint(f\"  Matches   : {len(results_c):,}\\n\")\nprint(f\"  {chr(82)+\"ANK\":<6} {chr(68)+\"OCUMENT ID\":<35} {chr(83)+\"IMILARITY\":>12}\")\nprint(\"  \" + \"â”€\"*55)\nfor rank,(did,sc) in enumerate(results_c[:15], 1):\n    print(f\"  {rank:<6} {did:<35} {sc:>12.6f}\")\nif len(results_c)>15:\n    print(f\"  ... +{len(results_c)-15} more\")\nprint(\"\\nâœ…  Task C done!\")\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# â”€â”€ TASK C: VISUALIZATIONS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\ntop_n     = min(20, len(results_c))\ntop_docs  = results_c[:top_n]\ntop_scores= [s for _,s in top_docs]\ntop_labels= [d.split('/')[-1] for d,_ in top_docs]\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 7))\nfig.patch.set_facecolor('#0d1117')\nfig.suptitle('ğŸŸ¢ Task C â€” Document Similarity Search',\n             fontsize=18, fontweight='bold', color='#3fb950')\n\n# Histogram of ALL similarity scores\nax1.hist(ALL_SIMS, bins=80, color='#4FC3F7', alpha=0.75, edgecolor='none', label='All docs')\nylim = ax1.get_ylim()\nax1.axvline(threshold, color='#FF6B6B', lw=2.5,\n            label=f'{PERCENTILE}th pct = {threshold:.4f}')\nax1.fill_betweenx([0, ylim[1]], threshold, ALL_SIMS.max(),\n                   alpha=0.2, color='#3fb950', label=f'{len(results_c):,} matches')\nax1.set_ylim(ylim)\nax1.set_title('Cosine Similarity Distribution', color='#3fb950', fontweight='bold')\nax1.set_xlabel('Cosine Similarity Score')\nax1.set_ylabel('# Documents')\nax1.legend(fontsize=9, framealpha=0.3)\n\n# Top-N matches bar chart\ngrad = plt.cm.YlGn(np.linspace(0.4, 1.0, top_n))\nbars = ax2.barh(top_labels[::-1], top_scores[::-1], color=grad, edgecolor='none')\nax2.set_title(f'Top {top_n} Most Similar Documents', color='#3fb950', fontweight='bold')\nax2.set_xlabel('Cosine Similarity Score')\nfor b, sc in zip(bars, top_scores[::-1]):\n    ax2.text(b.get_width()+0.001, b.get_y()+b.get_height()/2,\n             f'{sc:.4f}', va='center', color='white', fontsize=7)\n\nplt.tight_layout()\nplt.savefig('task_c_similarity.png', dpi=150, bbox_inches='tight', facecolor='#0d1117')\nplt.show()\nprint('âœ…  Saved: task_c_similarity.png')\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## âœ… Summary\n\n| Task | Method | Corpus | Output |\n|------|--------|--------|--------|\n| ğŸ”µ A | K-Means on L2-normalized TF-IDF | Reuters 10,788 docs | K clusters with top terms + PCA map |\n| ğŸŸ¡ B | Mean TF-IDF score per keyword | Reuters 10,788 docs | TOP / MEDIUM / BOTTOM classification |\n| ğŸŸ¢ C | Cosine similarity (no stopword removal) | Reuters 10,788 docs | Ranked list of similar documents |\n\n> ğŸ“ Plots saved: `task_a_clusters.png`, `task_b_keywords.png`, `task_c_similarity.png`\n"
  }
 ]
}